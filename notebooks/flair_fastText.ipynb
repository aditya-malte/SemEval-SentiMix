{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flair_fastText.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kV1gJzTbFBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3107d5ca-fe20-40d7-f189-5f899b91d0bd"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/pranaychandekar/fasttext-embeddings-with-flair.git\n",
        "%cd fasttext-embeddings-with-flair\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "'''\n",
        "\n",
        "# %cd fasttext-embeddings-with-flair\n",
        "# from fasttext_custom_embeddings_with_flair import FastTextEmbeddings \n",
        "# %cd ..\n",
        "# !ls\n",
        "\n",
        "# !pip install flair\n",
        "!pip install emoji\n",
        "!pip install --upgrade git+https://github.com/aditya-malte/flair.git\n",
        "# !pip install --upgrade git+https://github.com/zalandoresearch/flair.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 27.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=9fa4e38ac767f17d05e4985af2c9f51c78d16e56e9a2955a7ad603ac93cd7e9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "Collecting git+https://github.com/aditya-malte/flair.git\n",
            "  Cloning https://github.com/aditya-malte/flair.git to /tmp/pip-req-build-ycmy3x9q\n",
            "  Running command git clone -q https://github.com/aditya-malte/flair.git /tmp/pip-req-build-ycmy3x9q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c0/34033b2df7718b91c667bd259d5ce632ec3720198b7068c0ba6f6104ff89/pytest-5.3.5-py3-none-any.whl (235kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.4.0)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.1.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.22.1)\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (0.8.6)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (2.6.1)\n",
            "Collecting transformers>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5) (3.1.3)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.8.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (20.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (1.5.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (8.2.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.5) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair==0.4.5) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5) (1.9.0)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair==0.4.5) (1.11.10)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair==0.4.5) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.4.5) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.4.5) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair==0.4.5) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair==0.4.5) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair==0.4.5) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair==0.4.5) (1.14.10)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair==0.4.5) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair==0.4.5) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.3.0->flair==0.4.5) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->flair==0.4.5) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers>=2.3.0->flair==0.4.5) (0.15.2)\n",
            "Building wheels for collected packages: flair\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.4.5-cp36-none-any.whl size=122382 sha256=d3d2baaff1e40b6d1ebe754baae87f87ad6dd5959c056cd06c81074d15981964\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pttde01b/wheels/d6/a1/32/11154eec7bf752bef13fac491ec736c175a7a5db3cbeae1b6a\n",
            "Successfully built flair\n",
            "Building wheels for collected packages: segtok, sqlitedict, langdetect, mpld3, sacremoses\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=e7975cb1bc78f96949e2aa85f81f3d0b979fc916001a31cd0e5c878c4b0e9a53\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=7e577363794472295ffafecc4ef7dfa0c0eae3377dafe77fff8c0644db21499a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993459 sha256=ced0110187967c6fcd0947a6700e4fa42306e651d436b9a96f5c21792fa1d5cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=ba833e4b27c0e69b437672f43df9fbc283b76022da0a3a84d2ac3286a0703d85\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=7a93c9c0e64e17a894ed911387aa32dd21c2526c3d452f817e0fe7b49f68862d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built segtok sqlitedict langdetect mpld3 sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pluggy, pytest, segtok, deprecated, sqlitedict, sentencepiece, bpemb, langdetect, tokenizers, sacremoses, transformers, mpld3, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.7 flair-0.4.5 langdetect-1.0.7 mpld3-0.3 pluggy-0.13.1 pytest-5.3.5 sacremoses-0.0.38 segtok-1.5.7 sentencepiece-0.1.85 sqlitedict-1.6.0 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoIZ3nAUbfEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "b501c4a8-215f-44a4-b2f6-a3662039d7ec"
      },
      "source": [
        "import os\n",
        "import getpass\n",
        "repo_name = \"SemEval\"\n",
        "if repo_name not in os.listdir():\n",
        "  username = input(\"User: \")\n",
        "  password = getpass.getpass(prompt='Password: ', stream=None) \n",
        "  os.system(\"git clone https://\"+username+\":\"+password+\"@github.com/aditya-malte/\"+repo_name+\".git\")\n",
        "%cd {repo_name}\n",
        "from utils_text import PreProcess\n",
        "%cd ..\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: sushant21\n",
            "Password: ··········\n",
            "/content/SemEval\n",
            "/content\n",
            "sample_data  SemEval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8x1-XOib0sw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fb8dd4a1-982c-44cf-d162-63e0cb8cd8ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "!ln -s \"/gdrive/My Drive/SemEval_weights_data\" \"/content/\"\n",
        "drive_path = \"/content/SemEval_weights_data/data/\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "ln: failed to create symbolic link '/content/SemEval_weights_data': File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvBGZrZbb-0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "831fed8f-8d29-480c-b0e0-4c7a325f8bca"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import FastTextEmbeddings\n",
        "from flair.datasets import ClassificationCorpus\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from pathlib import Path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciq84YqBcGOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "33f51e9b-50a1-4dd7-f8fa-4e9b0f02d064"
      },
      "source": [
        "ft_embeddings = FastTextEmbeddings('./SemEval_weights_data/Fasttext_weights/fasttext_new_without_callback_200_300.model', use_local=True)\n",
        "\n",
        "# ft_embeddings = FastTextEmbeddings('./SemEval_weights_data/Fasttext_weights/fasttext_keyed_200_300.bin', use_local=True)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh98EHv4zdL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "39a46776-f11f-4b82-f4a7-82ed6120e9c1"
      },
      "source": [
        "sentence = Sentence('The quick brown fox jumps over a lazy dog.', use_tokenizer=True) \n",
        "ft_embeddings.embed(sentence)\n",
        "print(sentence[0],sentence[0].embedding)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: 1 The tensor([  0.1638,   0.5068,  -2.3955,   2.0717,   2.0919,   4.4568,  -3.3067,\n",
            "         -5.5832,   3.9505, -11.0079,   2.3050, -11.5865,  -2.2416,   0.5929,\n",
            "          8.0116,  -3.1285,  -3.2233,  -5.3596,  -9.2824,   3.3856,  -3.2870,\n",
            "         -5.5598,  -8.9675,  -1.7938,   9.8029,  -2.3496,   0.0400,   1.8635,\n",
            "         -1.3744,   3.8409,   6.8074,  -3.3770,  10.5450,  -1.4255,   0.7603,\n",
            "         -3.0151,  -1.1254,  -3.4443,  -4.7196,   0.9224,  -5.5308,   0.0721,\n",
            "          2.7204,  -7.2323,   4.8606,   2.0587,   4.2978, -11.6638,   9.1686,\n",
            "          0.5691,   6.9982,  -2.1607,  -2.8955,  -3.7815,   1.2945,   1.3324,\n",
            "         -8.5993,  -3.5236,   1.2009,   7.1049,   2.0126,  -2.4588,  -3.9297,\n",
            "          5.7855,  -5.6091,  -2.3870,   0.2266,  -1.6944,  -3.0092,  -4.7669,\n",
            "         -2.4971, -15.0845,   9.7064,   3.2709,  -6.5792,  -7.3738,   2.1603,\n",
            "         -3.8549,   0.1598,  -0.0297,   5.7134,  -7.0382,  -0.2526,  -0.2653,\n",
            "          3.3500,  -4.0828,  -5.1443,  -1.4003,   6.7863, -15.7291,   3.0092,\n",
            "         -6.9560,  -1.3144,  -3.1401,   4.8195,  12.8973,   4.4474,  -6.4887,\n",
            "         -4.9847,   1.7217,  -4.9033,  -3.6526,  -1.6791,  -2.2347,  -1.0572,\n",
            "        -10.6188,   7.8151,   2.5675,   3.4873, -12.5910,  -7.2399,   9.4831,\n",
            "         -4.0119,  -4.7752,  -6.5748,   1.7316,  -2.0751,   0.6916,  -6.2240,\n",
            "          0.5102,  -4.3758,   1.0327,  -7.4094,  -1.4281,  -4.3281,   3.0455,\n",
            "         -5.8070,  -8.6744,  -8.1448,   1.1892,  -3.1772,   4.2917,  -7.0216,\n",
            "         -0.7325,   5.2063,  -4.9467,   9.7951,  -6.1190,  -2.0821,   9.1548,\n",
            "         -0.4683,   6.8915,  -9.0640,   2.5341,  -2.0193,   3.5460,  -1.0381,\n",
            "          1.8999,   5.9871,   5.4524,   3.4423,  -7.8990,   2.8487,   5.3897,\n",
            "         -6.7467, -10.5840,   0.2415,  -9.0632,   1.8492,  -2.0277,   1.7028,\n",
            "         -4.7025,  -3.2194,  -9.8435,  -3.7896,   2.2523, -11.3434,   5.8499,\n",
            "         -4.8571,  -5.0029,  -5.8273,   3.7300, -11.2696,  -1.7370,   1.1452,\n",
            "          0.2349,  -4.1010,   6.9688,  11.1655,   1.4869,  -0.4749,  -6.6980,\n",
            "          2.4389,  -1.5055,   6.5607,   8.3564,   0.9356,  -9.8601,  -1.7218,\n",
            "         -3.1963,   4.6955,   0.6144,  -4.6868,   1.3588,   2.5040,  -2.1542,\n",
            "         -4.7228,   6.2365,  -0.9743,   9.4827], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKhwl-Qv8xbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "dcf2c0ee-299a-4a9b-a57e-0d0055bc9d4d"
      },
      "source": [
        "# load train/val data\n",
        "drive_path='./SemEval_weights_data/data/'\n",
        "df_train = pd.read_csv(drive_path+'processed_train.csv')\n",
        "df_test = pd.read_csv(drive_path+'processed_val.csv')\n",
        "df_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uid</th>\n",
              "      <th>text</th>\n",
              "      <th>lang_labels</th>\n",
              "      <th>url</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4330</td>\n",
              "      <td>nen á vist bolest vztek smutek zmatek osam ě l...</td>\n",
              "      <td>Eng O Eng Eng Eng Eng Hin Hin O Eng Eng O Hin ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>41616</td>\n",
              "      <td>@nehantics Haan yaar neha :pensive_face::pensi...</td>\n",
              "      <td>O Hin Hin Hin Hin O Hin Hin Hin Hin EMT Hin Hi...</td>\n",
              "      <td>https://t.co/5RSlSbZNtt</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6648</td>\n",
              "      <td>@RahulGandhi television media congress ke liye...</td>\n",
              "      <td>O Eng Eng Eng Eng Hin Hin Hin Hin O Hin Hin Hi...</td>\n",
              "      <td>https://t.co/HmH8M7PTaK</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2512</td>\n",
              "      <td>@AmitShah @narendramodi All India me nrc lagu ...</td>\n",
              "      <td>O Hin O Hin Hin Hin Eng Hin Hin Hin Eng Hin Hi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>610</td>\n",
              "      <td>@Nehr_who @TypoMantri @anjanaomkashyap Pagal h...</td>\n",
              "      <td>O Eng O Eng O Hin O Hin Hin Hin Hin O Eng Eng ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     uid  ... sentiment\n",
              "0   4330  ...   neutral\n",
              "1  41616  ...   neutral\n",
              "2   6648  ...  negative\n",
              "3   2512  ...  positive\n",
              "4    610  ...   neutral\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iq7LdJ3EqLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "7252092a-0357-46c1-de9d-89c1089550e3"
      },
      "source": [
        "# preprocess = PreProcess(sep_url=False, remove_url=True).preprocess\n",
        "preprocess = PreProcess(sep_url=False, remove_url=True, lowercase=True,\n",
        "               convert_emoji=True, solve_gaps=False, remove_punct = True).preprocess\n",
        "\n",
        "df_train[\"text\"] = df_train[\"text\"].apply(preprocess)\n",
        "df_test[\"text\"] = df_test[\"text\"].apply(preprocess)\n",
        "df_train.head()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uid</th>\n",
              "      <th>text</th>\n",
              "      <th>lang_labels</th>\n",
              "      <th>url</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4330</td>\n",
              "      <td>nen á vist bolest vztek smutek zmatek osam ě l...</td>\n",
              "      <td>Eng O Eng Eng Eng Eng Hin Hin O Eng Eng O Hin ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>41616</td>\n",
              "      <td>@nehantics haan yaar neha :pensive_face:pensiv...</td>\n",
              "      <td>O Hin Hin Hin Hin O Hin Hin Hin Hin EMT Hin Hi...</td>\n",
              "      <td>https://t.co/5RSlSbZNtt</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6648</td>\n",
              "      <td>@rahulgandhi television media congress ke liye...</td>\n",
              "      <td>O Eng Eng Eng Eng Hin Hin Hin Hin O Hin Hin Hi...</td>\n",
              "      <td>https://t.co/HmH8M7PTaK</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2512</td>\n",
              "      <td>@amitshah @narendramodi all india me nrc lagu ...</td>\n",
              "      <td>O Hin O Hin Hin Hin Eng Hin Hin Hin Eng Hin Hi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>610</td>\n",
              "      <td>@nehr_who @typomantri @anjanaomkashyap pagal h...</td>\n",
              "      <td>O Eng O Eng O Hin O Hin Hin Hin Hin O Eng Eng ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     uid  ... sentiment\n",
              "0   4330  ...   neutral\n",
              "1  41616  ...   neutral\n",
              "2   6648  ...  negative\n",
              "3   2512  ...  positive\n",
              "4    610  ...   neutral\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rxZFH6HIQQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f2fb520f-256e-49b2-cc49-6752da42bddd"
      },
      "source": [
        "## Convert train/test to flair readable format (__label__+label+text),\n",
        "## and save train/test/dev.csv\n",
        "## refer to https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f\n",
        "\n",
        "def classification_format_data(data, save_as):\n",
        "  data = data[['sentiment', 'text']].rename(columns={\"sentiment\":\"label\", \"text\":\"text\"})  \n",
        "  data['label'] = '__label__' + data['label'].astype(str)\n",
        "  data.to_csv(save_as, sep='\\t', index = False, header = False)\n",
        "  print(\"Saved to {}\".format(save_as))\n",
        "\n",
        "# df_train.columns.values\n",
        "classification_format_data(df_train, 'train.csv')\n",
        "classification_format_data(df_test, 'dev.csv')\n",
        "classification_format_data(df_test, 'test.csv')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved to train.csv\n",
            "Saved to dev.csv\n",
            "Saved to test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7nc9k5tF4by",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc33f09d-2ff6-446e-ec6b-97f01e23599c"
      },
      "source": [
        "# corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "corpus = ClassificationCorpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "\n",
        "\n",
        "# print(len(corpus.train))\n",
        "word_embeddings = [ft_embeddings]\n",
        "\n",
        "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
        "\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# trainer.train('./', max_epochs=10)\n",
        "log_dir='./SemEval_weights_data/flair_fastText/'\n",
        "trainer.train(log_dir, max_epochs=10, embeddings_storage_mode='gpu')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-02-08 07:07:42,496 Reading data from .\n",
            "2020-02-08 07:07:42,497 Train: train.csv\n",
            "2020-02-08 07:07:42,498 Dev: dev.csv\n",
            "2020-02-08 07:07:42,499 Test: test.csv\n",
            "2020-02-08 07:07:42,771 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13999/13999 [00:07<00:00, 1967.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-02-08 07:07:50,202 [b'neutral', b'negative', b'positive']\n",
            "2020-02-08 07:07:50,383 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:07:50,385 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentRNNEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): FastTextEmbeddings('./SemEval_weights_data/Fasttext_weights/fasttext_new_without_callback_200_300.model')\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=200, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512, batch_first=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-02-08 07:07:50,386 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:07:50,388 Corpus: \"Corpus: 13999 train + 3000 dev + 3000 test sentences\"\n",
            "2020-02-08 07:07:50,390 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:07:50,393 Parameters:\n",
            "2020-02-08 07:07:50,395  - learning_rate: \"0.1\"\n",
            "2020-02-08 07:07:50,397  - mini_batch_size: \"32\"\n",
            "2020-02-08 07:07:50,399  - patience: \"3\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-02-08 07:07:50,400  - anneal_factor: \"0.5\"\n",
            "2020-02-08 07:07:50,402  - max_epochs: \"10\"\n",
            "2020-02-08 07:07:50,404  - shuffle: \"True\"\n",
            "2020-02-08 07:07:50,406  - train_with_dev: \"False\"\n",
            "2020-02-08 07:07:50,407  - batch_growth_annealing: \"False\"\n",
            "2020-02-08 07:07:50,408 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:07:50,410 Model training base path: \"SemEval_weights_data/flair_fastText\"\n",
            "2020-02-08 07:07:50,411 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:07:50,413 Device: cuda:0\n",
            "2020-02-08 07:07:50,415 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:07:50,416 Embeddings storage mode: gpu\n",
            "2020-02-08 07:07:50,423 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:07:52,796 epoch 1 - iter 43/438 - loss 1.30564999 - samples/sec: 770.40\n",
            "2020-02-08 07:07:54,794 epoch 1 - iter 86/438 - loss 1.26030834 - samples/sec: 1007.08\n",
            "2020-02-08 07:07:56,759 epoch 1 - iter 129/438 - loss 1.25422620 - samples/sec: 1020.53\n",
            "2020-02-08 07:07:58,884 epoch 1 - iter 172/438 - loss 1.23251100 - samples/sec: 917.03\n",
            "2020-02-08 07:08:01,048 epoch 1 - iter 215/438 - loss 1.22396759 - samples/sec: 968.11\n",
            "2020-02-08 07:08:03,089 epoch 1 - iter 258/438 - loss 1.22017375 - samples/sec: 960.48\n",
            "2020-02-08 07:08:05,096 epoch 1 - iter 301/438 - loss 1.21267552 - samples/sec: 984.86\n",
            "2020-02-08 07:08:07,125 epoch 1 - iter 344/438 - loss 1.20632665 - samples/sec: 982.88\n",
            "2020-02-08 07:08:09,165 epoch 1 - iter 387/438 - loss 1.20554749 - samples/sec: 986.63\n",
            "2020-02-08 07:08:11,295 epoch 1 - iter 430/438 - loss 1.20331062 - samples/sec: 923.35\n",
            "2020-02-08 07:08:11,972 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:08:11,974 EPOCH 1 done: loss 1.2039 - lr 0.1000\n",
            "2020-02-08 07:08:15,539 DEV : loss 1.1116540431976318 - score 0.458\n",
            "2020-02-08 07:08:15,858 BAD EPOCHS (no improvement): 0\n",
            "2020-02-08 07:08:16,500 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:08:18,675 epoch 2 - iter 43/438 - loss 1.13612849 - samples/sec: 915.72\n",
            "2020-02-08 07:08:20,746 epoch 2 - iter 86/438 - loss 1.14097469 - samples/sec: 963.11\n",
            "2020-02-08 07:08:22,774 epoch 2 - iter 129/438 - loss 1.13783827 - samples/sec: 973.78\n",
            "2020-02-08 07:08:24,874 epoch 2 - iter 172/438 - loss 1.14990475 - samples/sec: 975.13\n",
            "2020-02-08 07:08:26,913 epoch 2 - iter 215/438 - loss 1.15919418 - samples/sec: 964.83\n",
            "2020-02-08 07:08:29,212 epoch 2 - iter 258/438 - loss 1.16397810 - samples/sec: 898.11\n",
            "2020-02-08 07:08:31,228 epoch 2 - iter 301/438 - loss 1.15939307 - samples/sec: 993.98\n",
            "2020-02-08 07:08:33,236 epoch 2 - iter 344/438 - loss 1.16823647 - samples/sec: 994.59\n",
            "2020-02-08 07:08:35,323 epoch 2 - iter 387/438 - loss 1.16589927 - samples/sec: 974.48\n",
            "2020-02-08 07:08:37,120 epoch 2 - iter 430/438 - loss 1.16553276 - samples/sec: 1001.26\n",
            "2020-02-08 07:08:37,818 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:08:37,822 EPOCH 2 done: loss 1.1663 - lr 0.1000\n",
            "2020-02-08 07:08:42,134 DEV : loss 1.097235083580017 - score 0.502\n",
            "2020-02-08 07:08:42,442 BAD EPOCHS (no improvement): 0\n",
            "2020-02-08 07:08:43,081 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:08:46,144 epoch 3 - iter 43/438 - loss 1.16762762 - samples/sec: 770.42\n",
            "2020-02-08 07:08:48,019 epoch 3 - iter 86/438 - loss 1.15154626 - samples/sec: 981.12\n",
            "2020-02-08 07:08:50,197 epoch 3 - iter 129/438 - loss 1.15792423 - samples/sec: 967.57\n",
            "2020-02-08 07:08:52,303 epoch 3 - iter 172/438 - loss 1.15823560 - samples/sec: 922.61\n",
            "2020-02-08 07:08:54,353 epoch 3 - iter 215/438 - loss 1.15929012 - samples/sec: 972.32\n",
            "2020-02-08 07:08:56,349 epoch 3 - iter 258/438 - loss 1.16366944 - samples/sec: 993.60\n",
            "2020-02-08 07:08:58,626 epoch 3 - iter 301/438 - loss 1.16331994 - samples/sec: 912.30\n",
            "2020-02-08 07:09:00,678 epoch 3 - iter 344/438 - loss 1.16183227 - samples/sec: 942.72\n",
            "2020-02-08 07:09:02,700 epoch 3 - iter 387/438 - loss 1.16171823 - samples/sec: 987.87\n",
            "2020-02-08 07:09:04,722 epoch 3 - iter 430/438 - loss 1.16294355 - samples/sec: 1002.54\n",
            "2020-02-08 07:09:05,431 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:09:05,435 EPOCH 3 done: loss 1.1631 - lr 0.1000\n",
            "2020-02-08 07:09:09,017 DEV : loss 1.0978020429611206 - score 0.4777\n",
            "2020-02-08 07:09:09,317 BAD EPOCHS (no improvement): 1\n",
            "2020-02-08 07:09:09,321 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:09:11,448 epoch 4 - iter 43/438 - loss 1.14621794 - samples/sec: 937.12\n",
            "2020-02-08 07:09:13,542 epoch 4 - iter 86/438 - loss 1.14152751 - samples/sec: 994.16\n",
            "2020-02-08 07:09:15,518 epoch 4 - iter 129/438 - loss 1.14857460 - samples/sec: 1002.56\n",
            "2020-02-08 07:09:17,294 epoch 4 - iter 172/438 - loss 1.15963964 - samples/sec: 1004.09\n",
            "2020-02-08 07:09:19,325 epoch 4 - iter 215/438 - loss 1.15587392 - samples/sec: 988.02\n",
            "2020-02-08 07:09:21,423 epoch 4 - iter 258/438 - loss 1.15899179 - samples/sec: 984.99\n",
            "2020-02-08 07:09:23,425 epoch 4 - iter 301/438 - loss 1.15653062 - samples/sec: 989.01\n",
            "2020-02-08 07:09:25,479 epoch 4 - iter 344/438 - loss 1.15689534 - samples/sec: 997.69\n",
            "2020-02-08 07:09:27,490 epoch 4 - iter 387/438 - loss 1.15245584 - samples/sec: 997.51\n",
            "2020-02-08 07:09:29,480 epoch 4 - iter 430/438 - loss 1.15585677 - samples/sec: 999.60\n",
            "2020-02-08 07:09:30,155 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:09:30,157 EPOCH 4 done: loss 1.1560 - lr 0.1000\n",
            "2020-02-08 07:09:33,657 DEV : loss 1.082037329673767 - score 0.5343\n",
            "2020-02-08 07:09:33,968 BAD EPOCHS (no improvement): 0\n",
            "2020-02-08 07:09:34,727 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:09:36,858 epoch 5 - iter 43/438 - loss 1.17708634 - samples/sec: 914.45\n",
            "2020-02-08 07:09:38,964 epoch 5 - iter 86/438 - loss 1.15705455 - samples/sec: 985.98\n",
            "2020-02-08 07:09:40,959 epoch 5 - iter 129/438 - loss 1.14745071 - samples/sec: 993.84\n",
            "2020-02-08 07:09:43,105 epoch 5 - iter 172/438 - loss 1.15200277 - samples/sec: 914.55\n",
            "2020-02-08 07:09:45,262 epoch 5 - iter 215/438 - loss 1.15647304 - samples/sec: 971.73\n",
            "2020-02-08 07:09:47,100 epoch 5 - iter 258/438 - loss 1.16192966 - samples/sec: 993.39\n",
            "2020-02-08 07:09:49,115 epoch 5 - iter 301/438 - loss 1.15497315 - samples/sec: 993.32\n",
            "2020-02-08 07:09:51,109 epoch 5 - iter 344/438 - loss 1.15195913 - samples/sec: 986.88\n",
            "2020-02-08 07:09:53,182 epoch 5 - iter 387/438 - loss 1.15597055 - samples/sec: 981.06\n",
            "2020-02-08 07:09:55,123 epoch 5 - iter 430/438 - loss 1.15979372 - samples/sec: 1031.19\n",
            "2020-02-08 07:09:55,821 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:09:55,822 EPOCH 5 done: loss 1.1615 - lr 0.1000\n",
            "2020-02-08 07:09:59,541 DEV : loss 1.057451605796814 - score 0.5287\n",
            "2020-02-08 07:10:00,023 BAD EPOCHS (no improvement): 1\n",
            "2020-02-08 07:10:00,030 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:10:01,910 epoch 6 - iter 43/438 - loss 1.14128890 - samples/sec: 923.57\n",
            "2020-02-08 07:10:03,966 epoch 6 - iter 86/438 - loss 1.14172523 - samples/sec: 992.96\n",
            "2020-02-08 07:10:06,046 epoch 6 - iter 129/438 - loss 1.13440860 - samples/sec: 990.80\n",
            "2020-02-08 07:10:08,055 epoch 6 - iter 172/438 - loss 1.14439411 - samples/sec: 983.28\n",
            "2020-02-08 07:10:10,022 epoch 6 - iter 215/438 - loss 1.14947847 - samples/sec: 965.90\n",
            "2020-02-08 07:10:12,018 epoch 6 - iter 258/438 - loss 1.14844928 - samples/sec: 988.96\n",
            "2020-02-08 07:10:14,191 epoch 6 - iter 301/438 - loss 1.15310842 - samples/sec: 967.74\n",
            "2020-02-08 07:10:16,239 epoch 6 - iter 344/438 - loss 1.15666196 - samples/sec: 973.23\n",
            "2020-02-08 07:10:18,362 epoch 6 - iter 387/438 - loss 1.15566377 - samples/sec: 973.76\n",
            "2020-02-08 07:10:20,396 epoch 6 - iter 430/438 - loss 1.15407626 - samples/sec: 1002.85\n",
            "2020-02-08 07:10:21,068 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:10:21,070 EPOCH 6 done: loss 1.1554 - lr 0.1000\n",
            "2020-02-08 07:10:24,620 DEV : loss 1.1148046255111694 - score 0.5343\n",
            "2020-02-08 07:10:24,919 BAD EPOCHS (no improvement): 2\n",
            "2020-02-08 07:10:25,493 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:10:27,649 epoch 7 - iter 43/438 - loss 1.11593986 - samples/sec: 911.69\n",
            "2020-02-08 07:10:29,543 epoch 7 - iter 86/438 - loss 1.13635718 - samples/sec: 995.63\n",
            "2020-02-08 07:10:31,559 epoch 7 - iter 129/438 - loss 1.13477880 - samples/sec: 998.54\n",
            "2020-02-08 07:10:33,830 epoch 7 - iter 172/438 - loss 1.13202800 - samples/sec: 908.00\n",
            "2020-02-08 07:10:35,820 epoch 7 - iter 215/438 - loss 1.14739713 - samples/sec: 1002.76\n",
            "2020-02-08 07:10:37,782 epoch 7 - iter 258/438 - loss 1.14405544 - samples/sec: 1002.80\n",
            "2020-02-08 07:10:39,785 epoch 7 - iter 301/438 - loss 1.14556144 - samples/sec: 1016.63\n",
            "2020-02-08 07:10:41,697 epoch 7 - iter 344/438 - loss 1.14554998 - samples/sec: 999.54\n",
            "2020-02-08 07:10:43,663 epoch 7 - iter 387/438 - loss 1.14787182 - samples/sec: 1000.25\n",
            "2020-02-08 07:10:45,611 epoch 7 - iter 430/438 - loss 1.14932301 - samples/sec: 1018.92\n",
            "2020-02-08 07:10:46,294 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:10:46,297 EPOCH 7 done: loss 1.1481 - lr 0.1000\n",
            "2020-02-08 07:10:49,870 DEV : loss 1.163525104522705 - score 0.5557\n",
            "2020-02-08 07:10:50,197 BAD EPOCHS (no improvement): 0\n",
            "2020-02-08 07:10:50,923 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:10:53,016 epoch 8 - iter 43/438 - loss 1.13111194 - samples/sec: 965.49\n",
            "2020-02-08 07:10:55,223 epoch 8 - iter 86/438 - loss 1.12887595 - samples/sec: 998.15\n",
            "2020-02-08 07:10:57,306 epoch 8 - iter 129/438 - loss 1.13825923 - samples/sec: 942.30\n",
            "2020-02-08 07:10:59,571 epoch 8 - iter 172/438 - loss 1.14669328 - samples/sec: 857.50\n",
            "2020-02-08 07:11:02,344 epoch 8 - iter 215/438 - loss 1.14602045 - samples/sec: 889.83\n",
            "2020-02-08 07:11:04,533 epoch 8 - iter 258/438 - loss 1.14833393 - samples/sec: 980.16\n",
            "2020-02-08 07:11:06,521 epoch 8 - iter 301/438 - loss 1.14250790 - samples/sec: 998.25\n",
            "2020-02-08 07:11:08,513 epoch 8 - iter 344/438 - loss 1.14218086 - samples/sec: 1001.50\n",
            "2020-02-08 07:11:10,554 epoch 8 - iter 387/438 - loss 1.14315333 - samples/sec: 1008.16\n",
            "2020-02-08 07:11:12,479 epoch 8 - iter 430/438 - loss 1.14785753 - samples/sec: 1045.48\n",
            "2020-02-08 07:11:13,159 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:11:13,164 EPOCH 8 done: loss 1.1497 - lr 0.1000\n",
            "2020-02-08 07:11:16,662 DEV : loss 1.0773218870162964 - score 0.5433\n",
            "2020-02-08 07:11:16,979 BAD EPOCHS (no improvement): 1\n",
            "2020-02-08 07:11:16,985 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:11:19,077 epoch 9 - iter 43/438 - loss 1.11661116 - samples/sec: 944.89\n",
            "2020-02-08 07:11:21,119 epoch 9 - iter 86/438 - loss 1.14986221 - samples/sec: 1014.27\n",
            "2020-02-08 07:11:23,103 epoch 9 - iter 129/438 - loss 1.14610275 - samples/sec: 1003.54\n",
            "2020-02-08 07:11:25,058 epoch 9 - iter 172/438 - loss 1.15476653 - samples/sec: 985.92\n",
            "2020-02-08 07:11:27,090 epoch 9 - iter 215/438 - loss 1.16958068 - samples/sec: 971.76\n",
            "2020-02-08 07:11:29,200 epoch 9 - iter 258/438 - loss 1.16704629 - samples/sec: 1006.51\n",
            "2020-02-08 07:11:31,162 epoch 9 - iter 301/438 - loss 1.17026913 - samples/sec: 1008.59\n",
            "2020-02-08 07:11:33,135 epoch 9 - iter 344/438 - loss 1.16608414 - samples/sec: 1003.97\n",
            "2020-02-08 07:11:34,932 epoch 9 - iter 387/438 - loss 1.16557213 - samples/sec: 997.62\n",
            "2020-02-08 07:11:36,990 epoch 9 - iter 430/438 - loss 1.16802287 - samples/sec: 995.38\n",
            "2020-02-08 07:11:37,658 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:11:37,660 EPOCH 9 done: loss 1.1680 - lr 0.1000\n",
            "2020-02-08 07:11:42,305 DEV : loss 1.1138882637023926 - score 0.5587\n",
            "2020-02-08 07:11:42,601 BAD EPOCHS (no improvement): 0\n",
            "2020-02-08 07:11:43,228 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:11:45,329 epoch 10 - iter 43/438 - loss 1.17969552 - samples/sec: 929.10\n",
            "2020-02-08 07:11:47,322 epoch 10 - iter 86/438 - loss 1.14059585 - samples/sec: 995.53\n",
            "2020-02-08 07:11:49,147 epoch 10 - iter 129/438 - loss 1.14473162 - samples/sec: 993.98\n",
            "2020-02-08 07:11:51,323 epoch 10 - iter 172/438 - loss 1.15048784 - samples/sec: 960.48\n",
            "2020-02-08 07:11:53,496 epoch 10 - iter 215/438 - loss 1.15681019 - samples/sec: 901.04\n",
            "2020-02-08 07:11:55,492 epoch 10 - iter 258/438 - loss 1.15535158 - samples/sec: 1001.28\n",
            "2020-02-08 07:11:57,538 epoch 10 - iter 301/438 - loss 1.15871092 - samples/sec: 994.94\n",
            "2020-02-08 07:11:59,778 epoch 10 - iter 344/438 - loss 1.15904396 - samples/sec: 910.80\n",
            "2020-02-08 07:12:01,759 epoch 10 - iter 387/438 - loss 1.16000233 - samples/sec: 999.62\n",
            "2020-02-08 07:12:03,724 epoch 10 - iter 430/438 - loss 1.16148991 - samples/sec: 1013.90\n",
            "2020-02-08 07:12:04,463 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:12:04,465 EPOCH 10 done: loss 1.1638 - lr 0.1000\n",
            "2020-02-08 07:12:07,968 DEV : loss 0.9906442165374756 - score 0.555\n",
            "2020-02-08 07:12:08,264 BAD EPOCHS (no improvement): 1\n",
            "2020-02-08 07:12:08,876 ----------------------------------------------------------------------------------------------------\n",
            "2020-02-08 07:12:08,881 Testing using best model ...\n",
            "2020-02-08 07:12:08,884 loading file SemEval_weights_data/flair_fastText/best-model.pt\n",
            "2020-02-08 07:12:12,748 0.5587\t0.5587\t0.5587\n",
            "2020-02-08 07:12:12,753 \n",
            "MICRO_AVG: acc 0.3876 - f1-score 0.5587\n",
            "MACRO_AVG: acc 0.3829 - f1-score 0.5395333333333333\n",
            "negative   tp: 663 - fp: 555 - fn: 227 - tn: 1555 - precision: 0.5443 - recall: 0.7449 - accuracy: 0.4588 - f1-score: 0.6290\n",
            "neutral    tp: 271 - fp: 244 - fn: 857 - tn: 1628 - precision: 0.5262 - recall: 0.2402 - accuracy: 0.1975 - f1-score: 0.3298\n",
            "positive   tp: 742 - fp: 525 - fn: 240 - tn: 1493 - precision: 0.5856 - recall: 0.7556 - accuracy: 0.4924 - f1-score: 0.6598\n",
            "2020-02-08 07:12:12,757 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [tensor(1.1117, device='cuda:0'),\n",
              "  tensor(1.0972, device='cuda:0'),\n",
              "  tensor(1.0978, device='cuda:0'),\n",
              "  tensor(1.0820, device='cuda:0'),\n",
              "  tensor(1.0575, device='cuda:0'),\n",
              "  tensor(1.1148, device='cuda:0'),\n",
              "  tensor(1.1635, device='cuda:0'),\n",
              "  tensor(1.0773, device='cuda:0'),\n",
              "  tensor(1.1139, device='cuda:0'),\n",
              "  tensor(0.9906, device='cuda:0')],\n",
              " 'dev_score_history': [0.458,\n",
              "  0.502,\n",
              "  0.4777,\n",
              "  0.5343,\n",
              "  0.5287,\n",
              "  0.5343,\n",
              "  0.5557,\n",
              "  0.5433,\n",
              "  0.5587,\n",
              "  0.555],\n",
              " 'test_score': 0.5587,\n",
              " 'train_loss_history': [1.2038704325619354,\n",
              "  1.1663222668105608,\n",
              "  1.163132126200689,\n",
              "  1.156004527796349,\n",
              "  1.161520498120077,\n",
              "  1.155444030892359,\n",
              "  1.148125756276797,\n",
              "  1.1497206400790716,\n",
              "  1.1679711962399417,\n",
              "  1.1637993973411926]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R6-zhB1IokI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}